{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 30.0,
  "eval_steps": 500,
  "global_step": 2670,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.5649717514124294,
      "grad_norm": 0.5216047167778015,
      "learning_rate": 0.0009816479400749064,
      "loss": 13.6796,
      "step": 50
    },
    {
      "epoch": 1.1242937853107344,
      "grad_norm": 0.6808360815048218,
      "learning_rate": 0.0009629213483146068,
      "loss": 4.1193,
      "step": 100
    },
    {
      "epoch": 1.689265536723164,
      "grad_norm": 0.40344205498695374,
      "learning_rate": 0.0009441947565543071,
      "loss": 3.7826,
      "step": 150
    },
    {
      "epoch": 2.248587570621469,
      "grad_norm": 0.4156434237957001,
      "learning_rate": 0.0009254681647940076,
      "loss": 2.9291,
      "step": 200
    },
    {
      "epoch": 2.8135593220338984,
      "grad_norm": 2.248161792755127,
      "learning_rate": 0.0009067415730337078,
      "loss": 2.3423,
      "step": 250
    },
    {
      "epoch": 3.3728813559322033,
      "grad_norm": 2.1633172035217285,
      "learning_rate": 0.0008880149812734082,
      "loss": 2.2812,
      "step": 300
    },
    {
      "epoch": 3.937853107344633,
      "grad_norm": 9.148646354675293,
      "learning_rate": 0.0008692883895131086,
      "loss": 2.1727,
      "step": 350
    },
    {
      "epoch": 4.497175141242938,
      "grad_norm": 4.8302226066589355,
      "learning_rate": 0.0008505617977528089,
      "loss": 2.117,
      "step": 400
    },
    {
      "epoch": 5.056497175141243,
      "grad_norm": 19.759654998779297,
      "learning_rate": 0.0008318352059925094,
      "loss": 2.0784,
      "step": 450
    },
    {
      "epoch": 5.621468926553672,
      "grad_norm": 6.375003814697266,
      "learning_rate": 0.0008131086142322097,
      "loss": 2.1084,
      "step": 500
    },
    {
      "epoch": 6.1807909604519775,
      "grad_norm": 1.8485558032989502,
      "learning_rate": 0.0007943820224719101,
      "loss": 2.077,
      "step": 550
    },
    {
      "epoch": 6.745762711864407,
      "grad_norm": 23.318910598754883,
      "learning_rate": 0.0007756554307116105,
      "loss": 2.0707,
      "step": 600
    },
    {
      "epoch": 7.305084745762712,
      "grad_norm": 6.433162212371826,
      "learning_rate": 0.0007569288389513109,
      "loss": 2.0483,
      "step": 650
    },
    {
      "epoch": 7.870056497175141,
      "grad_norm": 12.790802955627441,
      "learning_rate": 0.0007382022471910112,
      "loss": 2.0581,
      "step": 700
    },
    {
      "epoch": 8.429378531073446,
      "grad_norm": 5.8881611824035645,
      "learning_rate": 0.0007194756554307116,
      "loss": 2.0266,
      "step": 750
    },
    {
      "epoch": 8.994350282485875,
      "grad_norm": 13.546046257019043,
      "learning_rate": 0.000700749063670412,
      "loss": 2.101,
      "step": 800
    },
    {
      "epoch": 9.55367231638418,
      "grad_norm": 10.413365364074707,
      "learning_rate": 0.0006820224719101123,
      "loss": 2.0499,
      "step": 850
    },
    {
      "epoch": 10.112994350282486,
      "grad_norm": 6.914055824279785,
      "learning_rate": 0.0006632958801498128,
      "loss": 1.9997,
      "step": 900
    },
    {
      "epoch": 10.677966101694915,
      "grad_norm": 2.0503170490264893,
      "learning_rate": 0.0006445692883895131,
      "loss": 2.0176,
      "step": 950
    },
    {
      "epoch": 11.23728813559322,
      "grad_norm": 5.548604965209961,
      "learning_rate": 0.0006258426966292135,
      "loss": 2.0144,
      "step": 1000
    },
    {
      "epoch": 11.80225988700565,
      "grad_norm": 1.7684814929962158,
      "learning_rate": 0.0006071161048689139,
      "loss": 2.0002,
      "step": 1050
    },
    {
      "epoch": 12.361581920903955,
      "grad_norm": 2.0022270679473877,
      "learning_rate": 0.0005883895131086142,
      "loss": 1.9566,
      "step": 1100
    },
    {
      "epoch": 12.926553672316384,
      "grad_norm": 1.0665082931518555,
      "learning_rate": 0.0005696629213483147,
      "loss": 1.9769,
      "step": 1150
    },
    {
      "epoch": 13.485875706214689,
      "grad_norm": 21.14430046081543,
      "learning_rate": 0.000550936329588015,
      "loss": 1.9448,
      "step": 1200
    },
    {
      "epoch": 14.045197740112995,
      "grad_norm": 4.759395122528076,
      "learning_rate": 0.0005322097378277153,
      "loss": 1.9222,
      "step": 1250
    },
    {
      "epoch": 14.610169491525424,
      "grad_norm": 8.30626106262207,
      "learning_rate": 0.0005134831460674158,
      "loss": 1.9393,
      "step": 1300
    },
    {
      "epoch": 15.169491525423728,
      "grad_norm": 129.18453979492188,
      "learning_rate": 0.0004947565543071161,
      "loss": 1.9257,
      "step": 1350
    },
    {
      "epoch": 15.734463276836157,
      "grad_norm": 3.1038308143615723,
      "learning_rate": 0.00047602996254681644,
      "loss": 1.9414,
      "step": 1400
    },
    {
      "epoch": 16.293785310734464,
      "grad_norm": 5.907389163970947,
      "learning_rate": 0.00045730337078651683,
      "loss": 1.911,
      "step": 1450
    },
    {
      "epoch": 16.858757062146893,
      "grad_norm": 3.5482990741729736,
      "learning_rate": 0.0004385767790262172,
      "loss": 1.9992,
      "step": 1500
    },
    {
      "epoch": 17.418079096045197,
      "grad_norm": 6.972030162811279,
      "learning_rate": 0.0004198501872659176,
      "loss": 1.9034,
      "step": 1550
    },
    {
      "epoch": 17.983050847457626,
      "grad_norm": 129.3945770263672,
      "learning_rate": 0.000401123595505618,
      "loss": 1.9287,
      "step": 1600
    },
    {
      "epoch": 18.54237288135593,
      "grad_norm": 4.867328643798828,
      "learning_rate": 0.0003823970037453184,
      "loss": 1.8914,
      "step": 1650
    },
    {
      "epoch": 19.10169491525424,
      "grad_norm": 85.9994125366211,
      "learning_rate": 0.0003636704119850187,
      "loss": 1.9125,
      "step": 1700
    },
    {
      "epoch": 19.666666666666668,
      "grad_norm": 7.787843227386475,
      "learning_rate": 0.0003449438202247191,
      "loss": 1.9216,
      "step": 1750
    },
    {
      "epoch": 20.225988700564972,
      "grad_norm": 7.19054651260376,
      "learning_rate": 0.0003262172284644195,
      "loss": 1.8957,
      "step": 1800
    },
    {
      "epoch": 20.7909604519774,
      "grad_norm": 24.459779739379883,
      "learning_rate": 0.0003074906367041199,
      "loss": 1.9121,
      "step": 1850
    },
    {
      "epoch": 21.350282485875706,
      "grad_norm": 36.58454895019531,
      "learning_rate": 0.00028876404494382026,
      "loss": 1.891,
      "step": 1900
    },
    {
      "epoch": 21.915254237288135,
      "grad_norm": 9.061433792114258,
      "learning_rate": 0.00027003745318352065,
      "loss": 1.9251,
      "step": 1950
    },
    {
      "epoch": 22.47457627118644,
      "grad_norm": 6.582621097564697,
      "learning_rate": 0.000251310861423221,
      "loss": 1.904,
      "step": 2000
    },
    {
      "epoch": 23.033898305084747,
      "grad_norm": 17.73534393310547,
      "learning_rate": 0.00023258426966292134,
      "loss": 1.8887,
      "step": 2050
    },
    {
      "epoch": 23.598870056497177,
      "grad_norm": 132.4635772705078,
      "learning_rate": 0.00021385767790262173,
      "loss": 1.9097,
      "step": 2100
    },
    {
      "epoch": 24.15819209039548,
      "grad_norm": 6.4976043701171875,
      "learning_rate": 0.00019513108614232212,
      "loss": 1.8896,
      "step": 2150
    },
    {
      "epoch": 24.72316384180791,
      "grad_norm": 6.989027976989746,
      "learning_rate": 0.00017640449438202248,
      "loss": 1.901,
      "step": 2200
    },
    {
      "epoch": 25.282485875706215,
      "grad_norm": 32.35511779785156,
      "learning_rate": 0.00015767790262172286,
      "loss": 1.8883,
      "step": 2250
    },
    {
      "epoch": 25.847457627118644,
      "grad_norm": 11.963434219360352,
      "learning_rate": 0.00013895131086142322,
      "loss": 1.9116,
      "step": 2300
    },
    {
      "epoch": 26.406779661016948,
      "grad_norm": 5.1557111740112305,
      "learning_rate": 0.0001202247191011236,
      "loss": 1.882,
      "step": 2350
    },
    {
      "epoch": 26.971751412429377,
      "grad_norm": 3.491213798522949,
      "learning_rate": 0.00010149812734082397,
      "loss": 1.9017,
      "step": 2400
    },
    {
      "epoch": 27.531073446327685,
      "grad_norm": 4.085413455963135,
      "learning_rate": 8.277153558052434e-05,
      "loss": 1.8816,
      "step": 2450
    },
    {
      "epoch": 28.09039548022599,
      "grad_norm": 93.93626403808594,
      "learning_rate": 6.404494382022472e-05,
      "loss": 1.8846,
      "step": 2500
    },
    {
      "epoch": 28.65536723163842,
      "grad_norm": 1.5953890085220337,
      "learning_rate": 4.531835205992509e-05,
      "loss": 1.8973,
      "step": 2550
    },
    {
      "epoch": 29.214689265536723,
      "grad_norm": 17.9436092376709,
      "learning_rate": 2.659176029962547e-05,
      "loss": 1.8831,
      "step": 2600
    },
    {
      "epoch": 29.779661016949152,
      "grad_norm": 31.166210174560547,
      "learning_rate": 7.865168539325843e-06,
      "loss": 1.9037,
      "step": 2650
    }
  ],
  "logging_steps": 50,
  "max_steps": 2670,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 30,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.909718484549632e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
